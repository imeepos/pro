# 舆情大屏幕系统后端 API V1 版本规划

## 概述
为舆情大屏幕系统提供后端 API 支持，包括页面管理、舆情数据查询、实时数据推送、历史数据回放等功能。

## 技术栈（已确认）✅
- 后端框架: NestJS + TypeScript
- 数据库:
  - **MongoDB**: 存储爬取的原始数据（原始 HTML、未清洗数据）
  - **PostgreSQL**: 存储清洗后的结构化数据（舆情分析结果、页面配置）
- ORM: **TypeORM** (PostgreSQL) + **Mongoose** (MongoDB)
- 缓存: Redis (热点数据缓存、Session 存储)
- 实时通信: WebSocket (使用 @nestjs/websockets)
- 文件存储: **MinIO** (导出文件、图片、附件)
- 消息队列: **RabbitMQ** (数据采集任务、数据处理管道)
- 数据采集: **自定义爬虫** (基于 Puppeteer / Cheerio)

## 核心功能模块

### 1. 页面管理模块 (Pages Module)
管理大屏页面配置，支持 CRUD、复制、设置默认页面等操作。

#### 数据模型
```typescript
// page.entity.ts
export interface Page {
  id: string;                    // UUID
  name: string;                  // 页面名称
  description?: string;          // 页面描述
  screenSize: '1920x1080' | '4K' | 'ultrawide' | 'custom';
  customSize?: {
    width: number;
    height: number;
  };
  isDefault: boolean;            // 是否默认页面
  createdAt: Date;
  updatedAt: Date;
  config: PageConfig;            // 页面配置（JSON 存储）
}

export interface PageConfig {
  grid: {
    columns: number;             // 网格列数
    rows: number;                // 网格行数
  };
  components: ComponentInstance[];
}

export interface ComponentInstance {
  id: string;
  componentType: string;         // 组件类型
  position: { x: number; y: number };
  size: { w: number; h: number };
  zIndex?: number;
}
```

#### API 接口
```typescript
// 获取页面列表
GET /api/pages
Response: Page[]

// 获取单个页面
GET /api/pages/:id
Response: Page

// 创建页面
POST /api/pages
Body: {
  name: string;
  description?: string;
  screenSize: string;
  customSize?: { width: number; height: number };
  config: PageConfig;
}
Response: Page

// 更新页面
PUT /api/pages/:id
Body: Partial<Page>
Response: Page

// 删除页面
DELETE /api/pages/:id
Response: { success: boolean }

// 复制页面
POST /api/pages/:id/duplicate
Body: { newName: string }
Response: Page

// 设置默认页面
POST /api/pages/:id/set-default
Response: { success: boolean }
```

---

### 2. 舆情数据模块 (Sentiment Module)

#### 2.1 实时统计数据
提供当前时刻的关键指标数据。

```typescript
GET /api/sentiment/statistics
Response: {
  totalCount: number;          // 总舆情数
  todayCount: number;          // 今日新增
  positiveCount: number;       // 正面舆情
  neutralCount: number;        // 中性舆情
  negativeCount: number;       // 负面舆情
  growthRate: number;          // 增长率 (%)
  hotTopicsCount: number;      // 热点话题数
  alertCount: number;          // 预警数量
}
```

#### 2.2 趋势数据
提供时间序列趋势数据。

```typescript
GET /api/sentiment/trend
Query: {
  timeRange: 'hour' | 'day' | 'week' | 'month';
  startTime?: string;         // ISO 8601 格式
  endTime?: string;
}
Response: {
  timestamps: string[];       // 时间戳数组
  positive: number[];
  neutral: number[];
  negative: number[];
  total: number[];
}
```

#### 2.3 分类分布数据（饼图）
```typescript
GET /api/sentiment/category-distribution
Response: {
  categories: Array<{
    name: string;             // 分类名称
    value: number;            // 数量
    percentage: number;       // 百分比
  }>;
}
```

#### 2.4 热点话题排行（柱状图）
```typescript
GET /api/sentiment/hot-topics
Query: {
  limit?: number;             // 默认 10
}
Response: {
  topics: Array<{
    name: string;             // 话题名称
    count: number;            // 讨论数
    sentiment: 'positive' | 'neutral' | 'negative';
    trend: 'up' | 'down' | 'stable';
  }>;
}
```

#### 2.5 地理分布数据（地图）
```typescript
GET /api/sentiment/geo-distribution
Response: {
  regions: Array<{
    province: string;         // 省份
    city?: string;            // 城市
    count: number;            // 舆情数量
    sentiment: {
      positive: number;
      neutral: number;
      negative: number;
    };
    coordinates: [number, number];  // 经纬度
  }>;
}
```

#### 2.6 词云数据
```typescript
GET /api/sentiment/word-cloud
Query: {
  limit?: number;             // 默认 100
}
Response: {
  words: Array<{
    text: string;
    value: number;            // 词频
    sentiment?: 'positive' | 'neutral' | 'negative';
  }>;
}
```

#### 2.7 舆情列表（滚动列表）
```typescript
GET /api/sentiment/news
Query: {
  page?: number;
  pageSize?: number;
  sentiment?: 'positive' | 'neutral' | 'negative';
  keyword?: string;
  startTime?: string;
  endTime?: string;
}
Response: {
  total: number;
  page: number;
  pageSize: number;
  items: Array<{
    id: string;
    title: string;
    content: string;
    source: string;           // 来源
    author?: string;
    publishTime: string;
    sentiment: 'positive' | 'neutral' | 'negative';
    sentimentScore: number;   // 情感得分 [-1, 1]
    tags: string[];
    readCount?: number;
    commentCount?: number;
    shareCount?: number;
  }>;
}
```

#### 2.8 预警信息
```typescript
GET /api/sentiment/alerts
Query: {
  severity?: 'high' | 'medium' | 'low';
  isResolved?: boolean;
}
Response: {
  alerts: Array<{
    id: string;
    title: string;
    description: string;
    severity: 'high' | 'medium' | 'low';
    timestamp: string;
    isResolved: boolean;
    relatedNewsIds: string[];
  }>;
}
```

#### 2.9 媒体来源分布
```typescript
GET /api/sentiment/source-distribution
Response: {
  sources: Array<{
    name: string;             // 媒体名称
    count: number;            // 数量
    percentage: number;
  }>;
}
```

---

### 3. 历史数据模块 (History Module)
支持历史数据查询和回放功能。

#### 3.1 获取历史快照
```typescript
GET /api/history/snapshots
Query: {
  startTime: string;          // ISO 8601
  endTime: string;
  interval?: number;          // 采样间隔（秒），默认 300
}
Response: {
  snapshots: Array<{
    timestamp: string;
    statistics: any;          // 统计数据快照
    charts: any;              // 图表数据快照
    news: any[];              // 新闻列表快照
    alerts: any[];            // 预警信息快照
  }>;
}
```

#### 3.2 获取关键事件
```typescript
GET /api/history/events
Query: {
  startTime: string;
  endTime: string;
}
Response: {
  events: Array<{
    id: string;
    timestamp: string;
    type: string;
    description: string;
    severity: 'high' | 'medium' | 'low';
    relatedData: any;
  }>;
}
```

---

### 4. 导出模块 (Export Module)

#### 4.1 导出 Excel 数据
```typescript
POST /api/export/excel
Body: {
  startTime: string;
  endTime: string;
  filters?: {
    sentiment?: string;
    source?: string;
    keyword?: string;
  };
  includeCharts?: boolean;    // 是否包含图表数据
}
Response: {
  downloadUrl: string;        // 文件下载链接
  fileSize: number;
  expiresAt: string;          // 过期时间
}
```

#### 4.2 导出 PDF 报告
```typescript
POST /api/export/pdf
Body: {
  pageId: string;             // 大屏页面 ID
  includeScreenshot?: boolean;
  timeRange?: {
    startTime: string;
    endTime: string;
  };
}
Response: {
  downloadUrl: string;
  fileSize: number;
  expiresAt: string;
}
```

---

### 5. WebSocket 实时推送模块

#### 连接端点
```
ws://xxx:3000/ws
```

#### 消息格式
```typescript
// 客户端订阅
{
  action: 'subscribe',
  channels: ['statistics', 'news', 'alerts']
}

// 服务端推送
{
  channel: 'statistics',
  data: {
    // 统计数据
  },
  timestamp: string
}

{
  channel: 'news',
  data: {
    // 新舆情数据
  }
}

{
  channel: 'alerts',
  data: {
    // 新预警信息
  }
}
```

#### 支持的频道
- `statistics`: 实时统计数据更新
- `news`: 新舆情数据推送
- `alerts`: 预警信息推送
- `trend`: 趋势数据更新
- `hot-topics`: 热点话题更新

---

## 数据库设计（已确认）✅

### 混合数据库架构
```
爬虫采集 → MongoDB (原始数据) → 数据清洗 → PostgreSQL (结构化数据) → API 查询
```

### 数据流转说明
1. **MongoDB（原始数据层）**
   - 存储爬虫抓取的原始 HTML
   - 存储未清洗的舆情数据
   - 作为数据湖，保留完整数据
   - 支持数据回溯和重新处理

2. **PostgreSQL（业务数据层）**
   - 存储清洗后的结构化舆情数据
   - 存储页面配置、用户设置
   - 存储预警信息、统计结果
   - 提供高性能查询和聚合

### 核心表结构（PostgreSQL）

#### pages 表
```sql
CREATE TABLE pages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  description TEXT,
  screen_size VARCHAR(50),
  custom_size JSONB,
  is_default BOOLEAN DEFAULT FALSE,
  config JSONB NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### sentiment_news 表（PostgreSQL - 清洗后的结构化数据）
```sql
CREATE TABLE sentiment_news (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  raw_data_id VARCHAR(24),              -- MongoDB 原始数据 ObjectId
  title VARCHAR(500) NOT NULL,
  content TEXT NOT NULL,
  source VARCHAR(255) NOT NULL,
  author VARCHAR(255),
  publish_time TIMESTAMP NOT NULL,
  crawl_time TIMESTAMP NOT NULL,
  sentiment VARCHAR(20) NOT NULL,       -- 'positive' | 'neutral' | 'negative'
  sentiment_score DECIMAL(3,2),         -- -1.00 到 1.00
  tags TEXT[],                          -- PostgreSQL 数组类型
  province VARCHAR(50),
  city VARCHAR(50),
  coordinates POINT,                    -- PostgreSQL 地理类型
  read_count INTEGER DEFAULT 0,
  comment_count INTEGER DEFAULT 0,
  share_count INTEGER DEFAULT 0,
  source_url TEXT NOT NULL,
  is_processed BOOLEAN DEFAULT TRUE,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_sentiment_news_publish_time ON sentiment_news(publish_time DESC);
CREATE INDEX idx_sentiment_news_sentiment ON sentiment_news(sentiment);
CREATE INDEX idx_sentiment_news_source ON sentiment_news(source);
CREATE INDEX idx_sentiment_news_province ON sentiment_news(province);
```

### MongoDB Collections（原始数据层）

#### raw_news_data（原始爬虫数据）
```typescript
// Mongoose Schema
{
  _id: ObjectId,
  sourceUrl: string,                    // 来源 URL
  sourceDomain: string,                 // 来源域名
  crawlTime: Date,                      // 爬取时间
  rawHtml: string,                      // 原始 HTML
  httpStatus: number,                   // HTTP 状态码
  headers: Object,                      // HTTP 响应头
  cookies: Object,                      // Cookies
  screenshot?: string,                  // 截图文件路径（MinIO）

  // 初步提取的字段（未清洗）
  extractedData: {
    title?: string,
    content?: string,
    author?: string,
    publishTime?: string,               // 原始时间字符串
    tags?: string[],
    images?: string[],
    videos?: string[]
  },

  // 爬虫元数据
  crawler: {
    name: string,                       // 爬虫名称
    version: string,
    taskId: string,                     // 任务 ID
    retryCount: number
  },

  // 处理状态
  processingStatus: 'pending' | 'processing' | 'completed' | 'failed',
  processedAt?: Date,
  processingError?: string,

  // 关联的 PostgreSQL 记录
  sentimentNewsId?: string              // UUID
}
```

#### crawl_tasks（爬虫任务队列）
```typescript
// Mongoose Schema
{
  _id: ObjectId,
  taskType: 'url' | 'keyword' | 'batch',
  priority: number,                     // 优先级（1-10）
  status: 'pending' | 'running' | 'completed' | 'failed',

  // 任务配置
  config: {
    urls?: string[],                    // URL 列表
    keywords?: string[],                // 关键词列表
    searchEngine?: string,              // 搜索引擎
    maxPages?: number,                  // 最大抓取页数
    depth?: number,                     // 爬取深度
    rateLimit?: number                  // 速率限制（请求/秒）
  },

  // 任务结果
  result: {
    totalUrls: number,
    successCount: number,
    failedCount: number,
    startTime?: Date,
    endTime?: Date,
    duration?: number,                  // 耗时（秒）
    errors?: Array<{
      url: string,
      error: string,
      timestamp: Date
    }>
  },

  createdAt: Date,
  updatedAt: Date
}
```

#### alerts 表
```sql
CREATE TABLE alerts (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title VARCHAR(255) NOT NULL,
  description TEXT,
  severity VARCHAR(20) NOT NULL,
  is_resolved BOOLEAN DEFAULT FALSE,
  related_news_ids UUID[],
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  resolved_at TIMESTAMP
);
```

---

## 数据采集与处理（已确认）✅

### 数据处理全流程
```
┌──────────────┐
│ 1. 任务创建  │ → 用户/定时任务 创建爬虫任务
└──────┬───────┘
       ↓
┌──────────────┐
│ 2. 任务队列  │ → RabbitMQ 管理爬虫任务队列
└──────┬───────┘
       ↓
┌──────────────┐
│ 3. 爬虫采集  │ → Puppeteer/Cheerio 抓取网页
└──────┬───────┘   存储原始 HTML 到 MongoDB
       ↓
┌──────────────┐
│ 4. 数据提取  │ → 从 HTML 中提取标题/内容/作者等
└──────┬───────┘   初步结构化（仍在 MongoDB）
       ↓
┌──────────────┐
│ 5. 数据清洗  │ → 去重、格式化、过滤无效数据
└──────┬───────┘
       ↓
┌──────────────┐
│ 6. 情感分析  │ → NLP 分析情感倾向
└──────┬───────┘   关键词提取、分类
       ↓
┌──────────────┐
│ 7. 入库 PG   │ → 将清洗后的数据写入 PostgreSQL
└──────┬───────┘   更新统计信息
       ↓
┌──────────────┐
│ 8. 实时推送  │ → WebSocket 推送新数据到前端
└──────────────┘
```

### 1. 爬虫模块 (Crawler Module) **【第一期：微博】**

#### 技术方案（已确认）✅
- **Playwright**: 爬取微博动态网页（替代 Puppeteer，性能更好）
- **代理池**: 暂不使用（先测试效果，如需要再添加）
- **User-Agent 轮换**: 模拟真实浏览器
- **Cookie 管理**: **必需**，使用 Playwright 扫码登录获取 Cookie

#### 微博爬虫详细方案

##### 微博数据来源选择
**方案 1: 微博热搜榜（推荐，第一期）**
- URL: `https://s.weibo.com/top/summary`
- 优点: 无需登录，反爬较弱，数据集中
- 可获取: 热搜话题、热度值、分类标签
- 数据量: 50条/次，适合实时监控

**方案 2: 微博搜索（第二期）**
- URL: `https://s.weibo.com/weibo?q={keyword}`
- 需要: Cookie（建议登录）
- 可获取: 搜索结果、微博内容、评论数、转发数
- 数据量: 可配置，适合关键词监控

**方案 3: 微博用户主页（第三期）**
- URL: `https://weibo.com/u/{uid}`
- 需要: 登录 Cookie
- 可获取: 用户微博、粉丝数、互动数据
- 数据量: 大，适合特定用户监控

##### 微博热搜爬虫实现（第一期）**【使用 Playwright】**

```typescript
// weibo-hotrank.crawler.ts
import { Injectable, Logger } from '@nestjs/common';
import { chromium, Browser, Page } from 'playwright';

@Injectable()
export class WeiboHotRankCrawler {
  private readonly logger = new Logger(WeiboHotRankCrawler.name);
  private browser: Browser;

  async init() {
    this.browser = await chromium.launch({
      headless: true,
      args: [
        '--disable-blink-features=AutomationControlled',
      ]
    });
  }

  async crawlHotRank(): Promise<WeiboHotTopic[]> {
    const page = await this.browser.newPage();

    try {
      // 设置 User-Agent
      await page.setExtraHTTPHeaders({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
      });

      // 访问热搜榜
      await page.goto('https://s.weibo.com/top/summary', {
        waitUntil: 'networkidle',
        timeout: 30000
      });

      // 等待热搜列表加载
      await page.waitForSelector('#pl_top_realtimehot table tbody tr', { timeout: 10000 });

      // 提取热搜数据
      const hotTopics = await page.evaluate(() => {
        const rows = document.querySelectorAll('#pl_top_realtimehot table tbody tr');
        const topics: any[] = [];

        rows.forEach((row, index) => {
          if (index === 0) return; // 跳过表头

          const link = row.querySelector('td.td-02 a');
          const hotValue = row.querySelector('td.td-02 span');
          const icon = row.querySelector('td.td-03 i');

          if (link) {
            topics.push({
              rank: index,
              title: link.textContent?.trim() || '',
              url: 'https://s.weibo.com' + link.getAttribute('href'),
              hotValue: hotValue?.textContent?.trim() || '0',
              category: icon?.className || 'normal',
              crawlTime: new Date().toISOString()
            });
          }
        });

        return topics;
      });

      this.logger.log(`成功爬取 ${hotTopics.length} 条微博热搜`);
      return hotTopics;

    } catch (error) {
      this.logger.error('微博热搜爬取失败', error);
      throw error;
    } finally {
      await page.close();
    }
  }

  // 爬取热搜话题详情（点击进入话题，获取微博内容）
  async crawlTopicDetail(topicUrl: string): Promise<WeiboPost[]> {
    const page = await this.browser.newPage();

    try {
      await page.setExtraHTTPHeaders({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
      });

      await page.goto(topicUrl, { waitUntil: 'networkidle', timeout: 30000 });
      await page.waitForSelector('.card-wrap', { timeout: 10000 });

      // 模拟滚动加载更多
      for (let i = 0; i < 3; i++) {
        await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
        await page.waitForTimeout(2000);
      }

      // 提取微博内容
      const posts = await page.evaluate(() => {
        const cards = document.querySelectorAll('.card-wrap');
        const result: any[] = [];

        cards.forEach((card) => {
          const contentEl = card.querySelector('.txt');
          const authorEl = card.querySelector('.name');
          const timeEl = card.querySelector('.from a');

          if (contentEl && authorEl) {
            result.push({
              content: contentEl.textContent?.trim() || '',
              author: authorEl.textContent?.trim() || '',
              publishTime: timeEl?.textContent?.trim() || '',
              repostCount: 0,  // 需要进一步解析
              commentCount: 0,
              likeCount: 0
            });
          }
        });

        return result;
      });

      this.logger.log(`成功爬取话题 ${topicUrl} 的 ${posts.length} 条微博`);
      return posts;

    } catch (error) {
      this.logger.error('微博话题详情爬取失败', error);
      throw error;
    } finally {
      await page.close();
    }
  }

  async destroy() {
    if (this.browser) {
      await this.browser.close();
    }
  }
}

export interface WeiboHotTopic {
  rank: number;
  title: string;
  url: string;
  hotValue: string;
  category: string;
  crawlTime: string;
}

export interface WeiboPost {
  content: string;
  author: string;
  publishTime: string;
  repostCount: number;
  commentCount: number;
  likeCount: number;
}
```

##### 微博反爬应对策略（已确认）✅

**1. User-Agent 轮换**
```typescript
const userAgents = [
  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
];

// 使用 Playwright 设置
await page.setExtraHTTPHeaders({
  'User-Agent': userAgents[Math.floor(Math.random() * userAgents.length)]
});
```

**2. 请求频率控制**
```typescript
// 每次请求间隔 3-5 秒（随机）
const delay = Math.random() * 2000 + 3000;
await page.waitForTimeout(delay);
```

**3. 隐藏自动化特征（Playwright 自带）**
```typescript
// Playwright 默认隐藏了 webdriver 特征，无需额外配置
// 但可以进一步优化：
await page.addInitScript(() => {
  // 覆盖 navigator.webdriver
  Object.defineProperty(navigator, 'webdriver', { get: () => false });

  // 覆盖 plugins
  Object.defineProperty(navigator, 'plugins', {
    get: () => [1, 2, 3, 4, 5]
  });
});
```

**4. 代理池（暂不使用）**
```typescript
// 第一期暂不使用代理池，先测试效果
// 如需要，可后续添加：
// const browser = await chromium.launch({
//   proxy: {
//     server: 'http://proxy-server:port',
//     username: 'user',
//     password: 'pass'
//   }
// });
```

**5. Cookie 管理（必需）✅**
```typescript
// 微博需要登录才能爬取数据
// Cookie 通过扫码登录获取，并持久化存储
const cookiePath = './cookies/weibo-cookies.json';

// 加载保存的 Cookie
if (fs.existsSync(cookiePath)) {
  const cookies = JSON.parse(fs.readFileSync(cookiePath, 'utf-8'));
  await context.addCookies(cookies);
}
```

##### 微博数据结构

```typescript
// MongoDB Schema - 微博原始数据
export interface WeiboRawData {
  _id: ObjectId;
  type: 'hotrank' | 'search' | 'user';  // 数据类型

  // 热搜榜数据
  hotRank?: {
    rank: number;
    title: string;
    url: string;
    hotValue: string;
    category: string;
  };

  // 微博内容
  post?: {
    content: string;
    author: string;
    authorUid?: string;
    publishTime: string;
    repostCount: number;
    commentCount: number;
    likeCount: number;
    images?: string[];
    videos?: string[];
  };

  sourceUrl: string;
  crawlTime: Date;
  rawHtml?: string;

  // 处理状态
  processingStatus: 'pending' | 'processing' | 'completed' | 'failed';
  sentimentNewsId?: string;
}
```

#### 爬虫配置（已确认）✅
```typescript
// crawler.config.ts
export const weiboConfig = {
  // 并发控制
  concurrency: 2,                       // 同时运行的爬虫数（微博建议低并发）
  requestDelay: 3000,                   // 请求间隔（毫秒）

  // 重试策略
  maxRetries: 3,
  retryDelay: 5000,

  // 超时设置
  timeout: 30000,

  // 代理配置
  useProxy: false,                      // ✅ 第一期不使用代理

  // Cookie 配置
  useCookie: false,                     // ✅ 不使用登录
  cookiePath: './cookies/weibo.json',

  // 定时任务
  schedule: {
    hotRank: '*/10 * * * *',            // 每 10 分钟爬取热搜
    topicDetail: '*/30 * * * * *'       // 每 30 分钟爬取话题详情
  }
};
```

#### 爬虫任务管理
```typescript
// crawler.service.ts
@Injectable()
export class CrawlerService {
  // 创建爬虫任务
  async createTask(config: CrawlTaskConfig): Promise<CrawlTask>

  // 执行单个 URL 爬取
  async crawlUrl(url: string, parser: string): Promise<RawNewsData>

  // 执行关键词搜索爬取
  async crawlByKeyword(keyword: string, searchEngine: string): Promise<RawNewsData[]>

  // 批量爬取
  async crawlBatch(urls: string[]): Promise<void>

  // 停止任务
  async stopTask(taskId: string): Promise<void>
}
```

### 2. 数据清洗模块 (Data Cleaning Module)

#### 清洗任务
- 去除 HTML 标签和无效字符
- 统一时间格式（ISO 8601）
- 提取地理位置信息（正则匹配省市名）
- 去重（基于标题相似度或 URL）
- 过滤垃圾内容（广告、无效文本）

```typescript
// data-cleaning.service.ts
@Injectable()
export class DataCleaningService {
  // 清洗单条原始数据
  async cleanRawData(rawData: RawNewsData): Promise<CleanedNewsData>

  // 批量清洗
  async cleanBatch(rawDataIds: string[]): Promise<void>

  // 去重检测
  async checkDuplicate(title: string, sourceUrl: string): Promise<boolean>

  // 提取地理位置
  extractLocation(content: string): { province?: string; city?: string }
}
```

### 3. 情感分析模块 (Sentiment Analysis Module) **【已确认：使用大模型】**

#### 技术方案：大模型 API 调用
**主要模型**: DeepSeek（默认）
**支持切换**: OpenAI、Claude、通义千问、文心一言等其他大模型

#### 架构设计（策略模式）
```typescript
// llm-provider.interface.ts
export interface LLMProvider {
  name: string;
  analyze(text: string): Promise<SentimentAnalysisResult>;
  extractKeywords(text: string): Promise<string[]>;
}

export interface SentimentAnalysisResult {
  sentiment: 'positive' | 'neutral' | 'negative';
  score: number;              // -1.00 到 1.00
  confidence: number;         // 0 到 1（置信度）
  keywords: string[];
  reasoning?: string;         // 模型推理过程（可选）
}

// deepseek.provider.ts
@Injectable()
export class DeepSeekProvider implements LLMProvider {
  name = 'deepseek';
  private apiKey: string;
  private baseUrl = 'https://api.deepseek.com/v1';

  async analyze(text: string): Promise<SentimentAnalysisResult> {
    const prompt = `
请分析以下文本的情感倾向，并提取关键词：

文本：${text}

请按照以下 JSON 格式返回：
{
  "sentiment": "positive/neutral/negative",
  "score": -1.00 到 1.00 的数值,
  "confidence": 0 到 1 的置信度,
  "keywords": ["关键词1", "关键词2", ...],
  "reasoning": "简短的分析理由"
}
`;

    const response = await axios.post(
      `${this.baseUrl}/chat/completions`,
      {
        model: 'deepseek-chat',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3,
        response_format: { type: 'json_object' }
      },
      {
        headers: { 'Authorization': `Bearer ${this.apiKey}` }
      }
    );

    return JSON.parse(response.data.choices[0].message.content);
  }

  async extractKeywords(text: string): Promise<string[]> {
    // 实现关键词提取
  }
}

// openai.provider.ts
@Injectable()
export class OpenAIProvider implements LLMProvider {
  name = 'openai';
  // 类似实现
}

// sentiment-analysis.service.ts
@Injectable()
export class SentimentAnalysisService {
  private providers = new Map<string, LLMProvider>();
  private currentProvider: LLMProvider;

  constructor(
    private deepseekProvider: DeepSeekProvider,
    private openaiProvider: OpenAIProvider,
    // ... 其他 provider
    private configService: ConfigService
  ) {
    // 注册所有 provider
    this.registerProvider(deepseekProvider);
    this.registerProvider(openaiProvider);

    // 设置默认 provider
    const defaultProvider = this.configService.get('LLM_PROVIDER', 'deepseek');
    this.setProvider(defaultProvider);
  }

  registerProvider(provider: LLMProvider) {
    this.providers.set(provider.name, provider);
  }

  setProvider(name: string) {
    const provider = this.providers.get(name);
    if (!provider) {
      throw new Error(`Provider ${name} not found`);
    }
    this.currentProvider = provider;
  }

  async analyze(text: string): Promise<SentimentAnalysisResult> {
    return await this.currentProvider.analyze(text);
  }

  async analyzeBatch(texts: string[]): Promise<SentimentAnalysisResult[]> {
    // 批量分析，支持并发控制
    const results = [];
    for (const text of texts) {
      results.push(await this.analyze(text));
    }
    return results;
  }
}
```

#### 配置文件（.env）
```bash
# LLM 配置
LLM_PROVIDER=deepseek           # 默认使用的模型
LLM_BATCH_SIZE=10               # 批量处理大小
LLM_TIMEOUT=30000               # 请求超时（毫秒）
LLM_RETRY_TIMES=3               # 重试次数

# DeepSeek 配置
DEEPSEEK_API_KEY=sk-xxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
DEEPSEEK_MODEL=deepseek-chat

# OpenAI 配置（可选）
OPENAI_API_KEY=sk-xxxxx
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# 通义千问配置（可选）
QWEN_API_KEY=xxxxx
QWEN_BASE_URL=https://dashscope.aliyuncs.com/api/v1

# 文心一言配置（可选）
ERNIE_API_KEY=xxxxx
ERNIE_SECRET_KEY=xxxxx
```

#### API 接口（动态切换模型）
```typescript
// 切换模型
POST /api/sentiment/config/provider
Body: { provider: 'deepseek' | 'openai' | 'qwen' | 'ernie' }
Response: { success: boolean, currentProvider: string }

// 获取当前配置
GET /api/sentiment/config
Response: {
  currentProvider: string,
  availableProviders: string[],
  config: {
    batchSize: number,
    timeout: number,
    retryTimes: number
  }
}
```

#### 优势
✅ **高准确率**: 大模型理解能力强，准确率 90%+
✅ **灵活切换**: 支持多种模型，按需切换
✅ **上下文理解**: 理解复杂语义和讽刺
✅ **关键词提取**: 同时完成情感分析和关键词提取
✅ **成本控制**: 可根据预算选择不同模型

#### 注意事项
⚠️ **API 调用成本**: 需要考虑 API 费用
⚠️ **速率限制**: 需要实现速率控制和队列
⚠️ **降级策略**: API 失败时使用规则引擎兜底
⚠️ **缓存策略**: 相同文本缓存结果（Redis）

### 4. RabbitMQ 消息队列设计

#### 队列架构
```
┌─────────────────┐
│  crawl.tasks    │ → 爬虫任务队列（优先级队列）
└─────────────────┘

┌─────────────────┐
│  data.cleaning  │ → 数据清洗队列
└─────────────────┘

┌─────────────────┐
│  data.analysis  │ → 情感分析队列
└─────────────────┘

┌─────────────────┐
│  data.notify    │ → 实时通知队列（WebSocket 推送）
└─────────────────┘
```

#### 消息格式
```typescript
// 爬虫任务消息
{
  taskId: string,
  taskType: 'url' | 'keyword' | 'batch',
  priority: number,
  payload: {
    url?: string,
    keyword?: string,
    urls?: string[]
  },
  retry: {
    count: number,
    maxRetries: number
  }
}

// 数据清洗消息
{
  rawDataId: string,
  priority: number
}

// 情感分析消息
{
  newsId: string,
  text: string
}
```

---

## 部署架构（待确认）

### 开发环境
```
NestJS (xxx:3000)
PostgreSQL (xxx:5432)
MongoDB (xxx:27017)
Redis (xxx:6379)
```

### 生产环境（建议）
```
NestJS 服务（Docker 容器，支持水平扩展）
PostgreSQL（主从复制）
MongoDB 副本集
Redis 集群（缓存 + 消息队列）
Nginx 反向代理 + 负载均衡
```

---

## 安全与性能（待完善）

### 安全措施
- [ ] API 鉴权（JWT / API Key）**（待确认是否需要）**
- [ ] 请求频率限制（Rate Limiting）
- [ ] 数据加密（敏感信息）
- [ ] SQL 注入防护（使用 ORM）
- [ ] XSS 防护

### 性能优化
- [ ] Redis 缓存热点数据
- [ ] 数据库索引优化
- [ ] 分页查询（避免全表扫描）
- [ ] WebSocket 连接池管理
- [ ] CDN 加速静态资源

---

## 开发计划（任务拆分）

### 阶段 1: 项目初始化（无依赖）
- [ ] T1.1 检查现有 NestJS 项目配置
- [ ] T1.2 配置 TypeORM（PostgreSQL）
- [ ] T1.3 配置 Mongoose（MongoDB）
- [ ] T1.4 配置 Redis 连接
- [ ] T1.5 配置 RabbitMQ 连接（@nestjs/microservices）
- [ ] T1.6 配置 MinIO 客户端（minio npm 包）
- [ ] T1.7 配置 WebSocket 模块（@nestjs/websockets）
- [ ] T1.8 配置环境变量（.env）
- [ ] T1.9 配置 ESLint + Prettier（已有则检查）
- [ ] T1.10 配置 Docker Compose（PostgreSQL, MongoDB, Redis, RabbitMQ, MinIO）
- [ ] T1.11 创建基础目录结构（modules, entities, schemas）

**提交点**: 完成项目基础配置

### 阶段 2: 页面管理模块（依赖阶段1）
- [ ] T2.1 创建 Pages Module
- [ ] T2.2 定义 Page Entity / Schema
- [ ] T2.3 实现 Pages Service（CRUD 逻辑）
- [ ] T2.4 实现 Pages Controller（RESTful API）
- [ ] T2.5 实现页面复制功能
- [ ] T2.6 实现设置默认页面功能
- [ ] T2.7 添加数据校验（class-validator）
- [ ] T2.8 编写单元测试

**提交点**: 完成页面管理功能

### 阶段 3: Mock 数据服务（依赖阶段1，可与阶段2并行）
- [ ] T3.1 创建 Mock Module
- [ ] T3.2 生成 Mock 舆情数据（faker.js）
- [ ] T3.3 实现 Mock 统计数据接口
- [ ] T3.4 实现 Mock 趋势数据接口
- [ ] T3.5 实现 Mock 地理分布数据
- [ ] T3.6 实现 Mock 词云数据
- [ ] T3.7 实现 Mock 新闻列表接口
- [ ] T3.8 实现 Mock 预警数据接口

**提交点**: 完成 Mock 数据服务（前端可开始对接）

### 阶段 4: 舆情数据模块（依赖阶段3）
- [ ] T4.1 创建 Sentiment Module
- [ ] T4.2 定义 Sentiment Entity / Schema
- [ ] T4.3 实现统计数据接口
- [ ] T4.4 实现趋势数据接口
- [ ] T4.5 实现分类分布接口
- [ ] T4.6 实现热点话题接口
- [ ] T4.7 实现地理分布接口
- [ ] T4.8 实现词云数据接口
- [ ] T4.9 实现新闻列表接口（分页、筛选）
- [ ] T4.10 实现预警信息接口
- [ ] T4.11 实现媒体来源分布接口

**提交点**: 完成舆情数据查询功能

### 阶段 5: WebSocket 实时推送（依赖阶段4）
- [ ] T5.1 创建 WebSocket Gateway
- [ ] T5.2 实现客户端连接管理
- [ ] T5.3 实现频道订阅机制
- [ ] T5.4 实现统计数据实时推送
- [ ] T5.5 实现新闻数据实时推送
- [ ] T5.6 实现预警信息实时推送
- [ ] T5.7 实现心跳保活机制
- [ ] T5.8 实现断线重连逻辑

**提交点**: 完成 WebSocket 实时推送

### 阶段 6: 历史数据模块（依赖阶段4）
- [ ] T6.1 创建 History Module
- [ ] T6.2 设计历史快照数据结构
- [ ] T6.3 实现历史快照存储逻辑
- [ ] T6.4 实现历史快照查询接口
- [ ] T6.5 实现关键事件记录功能
- [ ] T6.6 实现关键事件查询接口
- [ ] T6.7 实现定时任务（生成历史快照）

**提交点**: 完成历史数据回放功能

### 阶段 7: 导出模块（依赖阶段4）
- [ ] T7.1 创建 Export Module
- [ ] T7.2 集成 Excel 导出库（exceljs）
- [ ] T7.3 实现 Excel 数据导出接口
- [ ] T7.4 集成 PDF 生成库（puppeteer）
- [ ] T7.5 实现 PDF 报告生成接口
- [ ] T7.6 实现文件临时存储和清理
- [ ] T7.7 实现文件下载接口

**提交点**: 完成导出功能

### 阶段 8: 爬虫模块（依赖阶段1）
- [ ] T8.1 创建 Crawler Module
- [ ] T8.2 定义 RawNewsData Schema（MongoDB）
- [ ] T8.3 定义 CrawlTask Schema（MongoDB）
- [ ] T8.4 安装爬虫依赖（puppeteer, cheerio, axios）
- [ ] T8.5 实现静态页面爬取（Cheerio）
- [ ] T8.6 实现动态页面爬取（Puppeteer）
- [ ] T8.7 实现网站解析器注册机制
- [ ] T8.8 实现 User-Agent 轮换
- [ ] T8.9 实现并发控制和速率限制
- [ ] T8.10 实现爬虫任务队列（RabbitMQ）
- [ ] T8.11 实现爬虫任务管理 API（创建、查询、停止）
- [ ] T8.12 实现爬虫错误处理和重试

**提交点**: 完成爬虫模块

### 阶段 9: 数据清洗模块（依赖阶段8）
- [ ] T9.1 创建 DataCleaning Module
- [ ] T9.2 实现 HTML 标签清理
- [ ] T9.3 实现时间格式标准化
- [ ] T9.4 实现地理位置提取（省市识别）
- [ ] T9.5 实现内容去重逻辑（基于相似度）
- [ ] T9.6 实现垃圾内容过滤
- [ ] T9.7 实现数据清洗队列消费者（RabbitMQ）
- [ ] T9.8 实现批量清洗接口

**提交点**: 完成数据清洗模块

### 阶段 10: 情感分析模块（依赖阶段9）**【使用 DeepSeek 大模型】**
- [ ] T10.1 创建 SentimentAnalysis Module
- [ ] T10.2 定义 LLMProvider 接口（策略模式）
- [ ] T10.3 实现 DeepSeekProvider（主要）
- [ ] T10.4 实现 OpenAIProvider（可选）
- [ ] T10.5 实现通义千问 Provider（可选）
- [ ] T10.6 实现文心一言 Provider（可选）
- [ ] T10.7 实现 Provider 注册和切换机制
- [ ] T10.8 实现批量分析接口（并发控制）
- [ ] T10.9 实现情感分析队列消费者（RabbitMQ）
- [ ] T10.10 实现 API 速率限制和重试
- [ ] T10.11 实现情感分析结果缓存（Redis）
- [ ] T10.12 实现降级策略（API 失败时使用规则引擎）
- [ ] T10.13 实现模型切换管理接口

**提交点**: 完成大模型情感分析模块

### 阶段 11: 数据处理流水线（依赖阶段8、9、10）
- [ ] T11.1 实现完整数据处理流水线（爬取→清洗→分析→入库）
- [ ] T11.2 实现 PostgreSQL 数据写入
- [ ] T11.3 实现 MongoDB 到 PostgreSQL 的数据映射
- [ ] T11.4 实现处理状态跟踪
- [ ] T11.5 实现失败数据重新处理机制
- [ ] T11.6 实现定时任务（Cron）- 定期爬取
- [ ] T11.7 实现数据处理监控和日志

**提交点**: 完成数据处理流水线

### 阶段 12: 性能优化与测试（依赖阶段2-11）
- [ ] T12.1 添加 Redis 缓存（统计数据、热点数据）
- [ ] T12.2 优化数据库查询（索引优化、慢查询分析）
- [ ] T12.3 优化 MongoDB 查询（索引、聚合优化）
- [ ] T12.4 实现 API 限流（Rate Limiting）
- [ ] T12.5 实现接口响应缓存
- [ ] T12.6 编写单元测试
- [ ] T12.7 编写集成测试
- [ ] T12.8 编写 E2E 测试
- [ ] T12.9 性能压测（Artillery / K6）
- [ ] T12.10 日志监控（Winston）
- [ ] T12.11 健康检查接口（/health）
- [ ] T12.12 Swagger API 文档生成

**提交点**: 完成测试和优化

### 执行顺序总结
```
阶段1（基础配置）
↓
阶段2（页面管理）| 阶段3（Mock 数据）| 阶段8（爬虫模块）（三者并行）
↓
阶段4（舆情数据查询）
↓
阶段5（WebSocket）| 阶段6（历史数据）| 阶段7（导出功能）（三者并行）
↓
阶段9（数据清洗）
↓
阶段10（情感分析）
↓
阶段11（数据流水线）
↓
阶段12（优化与测试）
```

**推荐优先级**：
1. **阶段1 → 阶段3** - 快速搭建 Mock 数据，让前端可以开始对接
2. **阶段2** - 页面管理功能，支持大屏编辑器
3. **阶段4、5、6、7** - 核心业务功能（查询、实时、历史、导出）
4. **阶段8、9、10、11** - 数据采集和处理流水线（可延后开发）

---

## 技术方案确认 ✅

### 已确认事项（全部完成）
1. ✅ **爬虫技术栈**: Playwright（替代 Puppeteer）
   - 数据来源: `https://s.weibo.com/top/summary`
   - 爬取频率: 每 10 分钟爬取热搜榜，每 30 分钟爬取话题详情
   - 反爬策略: User-Agent 轮换 + 频率控制（暂不使用代理池）

2. ✅ **情感分析**: 使用 DeepSeek 大模型
   - 主要模型: DeepSeek
   - API Key: `sk-38d1513afb404626b4be135b6124ce57`
   - 支持切换: OpenAI、Claude、通义千问、文心一言
   - 架构模式: 策略模式，可动态切换模型

3. ✅ **数据库架构**: 混合方案
   - MongoDB: 存储爬取的原始数据
   - PostgreSQL: 存储清洗后的结构化数据

4. ✅ **中间件技术栈**:
   - 消息队列: RabbitMQ
   - 缓存: Redis
   - 文件存储: MinIO

5. ✅ **用户鉴权**: 不需要鉴权，API 直接开放

6. ✅ **代理池**: 暂不使用，先测试效果

7. ✅ **历史数据**: 永久保存（不自动删除）

## 待确认事项 ⚠️

### 中优先级（可边开发边确认）
1. **数据量级预估**:
   - 微博热搜：每 10 分钟 50 条，每天 ~7200 条
   - 前端连接数预估？（WebSocket 并发）
   - 数据库存储容量规划？

### 中优先级
5. **部署方式**:
   - Docker Compose（单机部署，开发/测试环境）
   - Kubernetes（集群部署，生产环境）
   - **推荐初期使用 Docker Compose**

6. **Redis 缓存策略**:
   - 统计数据（TTL: 5分钟）
   - 热点舆情（TTL: 10分钟）
   - 地理分布数据（TTL: 30分钟）

7. **监控和日志**:
   - Winston（日志框架）
   - ELK Stack（可选，日志聚合）
   - Prometheus + Grafana（可选，性能监控）

### 低优先级（可延后确定）
8. **API 文档**: 使用 Swagger（@nestjs/swagger）
9. **健康检查**: 实现 /health 接口（K8s 就绪探针）
10. **数据备份策略**: PostgreSQL 定时备份、MongoDB 副本集

---

## 与前端对接约定

### API 基础路径
```
开发环境: http://xxx:3000/api
生产环境: https://api.example.com/api
```

### WebSocket 地址
```
开发环境: ws://xxx:3000/ws
生产环境: wss://api.example.com/ws
```

### 响应格式统一
```typescript
// 成功响应
{
  success: true,
  data: any,
  message?: string
}

// 错误响应
{
  success: false,
  error: {
    code: string,
    message: string,
    details?: any
  }
}
```

### 时间格式
统一使用 ISO 8601 格式：`2025-10-07T15:30:00Z`

### 分页参数
```typescript
{
  page: number;         // 页码，从 1 开始
  pageSize: number;     // 每页数量
  total: number;        // 总数
}
```

---

## 下一步行动 🚀

### 第一期核心目标（微博热搜 + DeepSeek 情感分析）

#### 快速启动路线图
```
Week 1: 基础配置 + Mock 数据
├─ Day 1-2: Docker Compose 配置（PostgreSQL, MongoDB, Redis, RabbitMQ, MinIO）
├─ Day 3-4: Mock 数据服务 → 前端可开始对接
└─ Day 5: 页面管理模块开始

Week 2-3: 核心业务功能
├─ 页面管理模块（CRUD）
├─ 舆情数据查询接口
├─ WebSocket 实时推送
└─ 历史数据和导出功能

Week 4-5: 微博爬虫 + 数据流水线 🔥
├─ 微博热搜爬虫实现
├─ 数据清洗模块
├─ DeepSeek 情感分析集成
├─ 完整数据流水线（爬取→清洗→分析→入库→推送）
└─ 定时任务配置

Week 6: 优化与上线
├─ 代理池配置（如需要）
├─ 性能优化和测试
├─ 监控和日志完善
└─ 生产环境部署
```

### 立即行动清单

#### 1. 准备工作（现在）
- [ ] 获取 DeepSeek API Key
- [ ] 确认是否需要代理池（建议先测试无代理）
- [ ] 确认用户鉴权需求（建议暂不实现）
- [ ] 准备服务器环境（Docker 环境）

#### 2. 第1周任务（基础设施）
- [ ] 配置 Docker Compose 全家桶
- [ ] 配置 NestJS 连接所有数据库和中间件
- [ ] 实现 Mock 数据服务（优先）
- [ ] 让前端开始对接开发

#### 3. 第2-3周任务（核心功能）
- [ ] 页面管理模块（支持编辑器）
- [ ] 舆情数据查询接口（9个接口）
- [ ] WebSocket 实时推送
- [ ] 历史数据查询和导出功能

#### 4. 第4-5周任务（数据采集）**【重点】**
- [ ] 微博热搜爬虫实现
- [ ] 话题详情爬取
- [ ] 数据清洗模块
- [ ] DeepSeek Provider 实现
- [ ] 完整数据流水线测试
- [ ] 定时任务配置

#### 5. 第6周任务（优化上线）
- [ ] 性能优化和压测
- [ ] 监控和日志
- [ ] 生产环境部署

### 技术准备清单

#### 依赖包（需安装）✅
```bash
# 爬虫相关（使用 Playwright）
pnpm add playwright
pnpm add cheerio axios                  # Cheerio 用于静态解析（可选）

# 数据库
pnpm add @nestjs/typeorm typeorm pg
pnpm add @nestjs/mongoose mongoose

# 中间件
pnpm add @nestjs/microservices amqplib amqp-connection-manager
pnpm add redis ioredis
pnpm add minio

# WebSocket
pnpm add @nestjs/websockets @nestjs/platform-socket.io socket.io

# 工具库
pnpm add @nestjs/schedule          # 定时任务
pnpm add @nestjs/config            # 配置管理
pnpm add class-validator class-transformer
pnpm add winston                   # 日志

# 大模型 SDK
pnpm add openai                    # DeepSeek 兼容 OpenAI SDK
pnpm add axios                     # HTTP 请求

# 导出功能
pnpm add exceljs                   # Excel 导出
pnpm add puppeteer-core            # PDF 生成（可选，也可用 Playwright）

# 工具库
pnpm add dayjs                     # 时间处理
pnpm add lodash                    # 工具函数
pnpm add @types/lodash -D

# 安装 Playwright 浏览器（必须）
npx playwright install chromium
```

#### 环境变量模板（.env）✅
```bash
# 服务配置
NODE_ENV=development
PORT=3000

# 数据库配置
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=sentiment

MONGODB_URI=mongodb://localhost:27017/sentiment
# 如果 MongoDB 有用户名密码：
# MONGODB_URI=mongodb://username:password@localhost:27017/sentiment

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# RabbitMQ
RABBITMQ_URL=amqp://guest:guest@localhost:5672

# MinIO
MINIO_ENDPOINT=localhost
MINIO_PORT=9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_USE_SSL=false
MINIO_BUCKET_NAME=sentiment-files

# DeepSeek API（已确认）✅
DEEPSEEK_API_KEY=sk-38d1513afb404626b4be135b6124ce57
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
DEEPSEEK_MODEL=deepseek-chat

# LLM 配置
LLM_PROVIDER=deepseek              # 默认使用的模型
LLM_BATCH_SIZE=10                  # 批量处理大小
LLM_TIMEOUT=30000                  # 请求超时（毫秒）
LLM_RETRY_TIMES=3                  # 重试次数

# OpenAI 配置（可选）
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# 爬虫配置（已确认）✅
CRAWLER_CONCURRENCY=2              # 并发数
CRAWLER_REQUEST_DELAY=3000         # 请求间隔（毫秒）
CRAWLER_USE_PROXY=false            # 不使用代理
CRAWLER_TIMEOUT=30000              # 超时时间
CRAWLER_MAX_RETRIES=3              # 最大重试次数

# 定时任务配置
CRAWLER_HOTRANK_SCHEDULE=*/10 * * * *     # 每 10 分钟爬取热搜
CRAWLER_TOPIC_SCHEDULE=*/30 * * * *       # 每 30 分钟爬取话题详情

# 历史数据配置（已确认）✅
HISTORY_DATA_RETENTION=永久          # 永久保存，不自动删除
HISTORY_SNAPSHOT_INTERVAL=300      # 历史快照间隔（秒）

# WebSocket 配置
WS_PORT=3000
WS_PATH=/ws

# 日志配置
LOG_LEVEL=info                     # debug | info | warn | error
LOG_DIR=./logs

# 安全配置（已确认：不需要鉴权）✅
AUTH_ENABLED=false                 # 不启用鉴权
```

### 关键里程碑

- **里程碑 1** (Week 1 末): 前端可开始对接 Mock 数据 ✅
- **里程碑 2** (Week 3 末): 核心查询功能上线 ✅
- **里程碑 3** (Week 5 末): 微博数据实时采集和分析 ✅ 🎯
- **里程碑 4** (Week 6 末): 系统优化完成，准备上线 ✅

---

*创建时间: 2025-10-08*
*最后更新: 2025-10-08*
